{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgtw6UUf8hO4",
        "outputId": "d7f7331f-57d9-4027-9db6-a0292cd41e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for qqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting animeface\n",
            "  Downloading animeface-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from animeface) (10.4.0)\n",
            "Downloading animeface-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: animeface\n",
            "Successfully installed animeface-2.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q qqdm\n",
        "!pip install animeface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id '1IJdJcXPNN_B7mMMPQO950szHZgFVvpT_' --output \"crypko_data.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emRAfAbG8ne2",
        "outputId": "1c414ae8-a66d-461d-8b36-148d9fa250f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1IJdJcXPNN_B7mMMPQO950szHZgFVvpT_\n",
            "From (redirected): https://drive.google.com/uc?id=1IJdJcXPNN_B7mMMPQO950szHZgFVvpT_&confirm=t&uuid=028d8d3f-1ec9-4366-be81-45dfc8493a12\n",
            "To: /content/crypko_data.zip\n",
            "100% 479M/479M [00:06<00:00, 75.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"crypko_data.zip\" -d \"./\""
      ],
      "metadata": {
        "id": "gRkscsu88not"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workspace_dir = '.'\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from qqdm.notebook import qqdm\n",
        "import shutil"
      ],
      "metadata": {
        "id": "ZCjvf0OL0-q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def same_seeds(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(2023)"
      ],
      "metadata": {
        "id": "L7a7g2hQ1EYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrypkoDataset(Dataset):\n",
        "    def __init__(self, fnames, transform=None):\n",
        "        self.transform = transform\n",
        "        self.fnames = fnames\n",
        "        self.num_samples = len(self.fnames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.fnames[idx]\n",
        "        img = torchvision.io.read_image(fname)\n",
        "        img = img.float() / 255.0\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "\n",
        "def get_dataset(root):\n",
        "    fnames = glob.glob(os.path.join(root, '*'))\n",
        "    compose = [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ]\n",
        "    transform = transforms.Compose(compose)\n",
        "    dataset = CrypkoDataset(fnames, transform)\n",
        "    return dataset\n",
        "\n",
        "dataset = get_dataset(os.path.join(workspace_dir, 'faces'))\n",
        "\n",
        "\n",
        "def visualize_samples(dataset, num_samples=16):\n",
        "    images = [dataset[i] for i in range(num_samples)]\n",
        "    grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.imshow(grid_img.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_samples(dataset)\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n"
      ],
      "metadata": {
        "id": "TMA7t4K91U8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.l1 = nn.Sequential(\n",
        "            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n",
        "            nn.BatchNorm1d(dim * 8 * 4 * 4),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.deconv_blocks = nn.Sequential(\n",
        "            nn.ConvTranspose2d(dim * 8, dim * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(dim * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(dim * 4, dim * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(dim * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(dim * 2, dim, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(dim, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.l1(x)\n",
        "        y = y.view(y.size(0), -1, 4, 4)\n",
        "        y = self.deconv_blocks(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "bGqM4VSG1VD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(in_channels, dim, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(dim, dim * 2, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.BatchNorm2d(dim * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(dim * 2, dim * 4, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.BatchNorm2d(dim * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(dim * 4, dim * 8, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.BatchNorm2d(dim * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(dim * 8, 1, kernel_size=4, stride=1, padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv_blocks(x)\n",
        "        return y.view(-1)"
      ],
      "metadata": {
        "id": "WHxjJGA71VG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "\n",
        "    def __init__(self, patience=5, min_delta=0.0):\n",
        "\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = loss\n",
        "            return\n",
        "\n",
        "        if self.best_loss - loss > self.min_delta:\n",
        "            self.best_loss = loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ],
      "metadata": {
        "id": "ds90BQCi1VKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "\n",
        "    def __init__(self, patience=5, min_delta=0.0):\n",
        "\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = loss\n",
        "            return\n",
        "\n",
        "        if self.best_loss - loss > self.min_delta:\n",
        "            self.best_loss = loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "batch_size = 64\n",
        "z_dim = 100\n",
        "lr = 0.0002\n",
        "n_epoch = 20\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "patience = 3\n",
        "min_delta = 0.0001\n",
        "log_dir = os.path.join(workspace_dir, 'logs')\n",
        "ckpt_dir = os.path.join(workspace_dir, 'checkpoints')\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "G = Generator(in_dim=z_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "G.train()\n",
        "D.train()\n",
        "criterion = nn.BCELoss()\n",
        "opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "opt_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "scheduler_D = torch.optim.lr_scheduler.StepLR(opt_D, step_size=10, gamma=0.5)\n",
        "scheduler_G = torch.optim.lr_scheduler.StepLR(opt_G, step_size=10, gamma=0.5)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
        "steps = 0\n",
        "for epoch in range(n_epoch):\n",
        "    progress_bar = qqdm(dataloader, desc=f'Epoch {epoch+1}/{n_epoch}')\n",
        "    epoch_loss_G = 0.0\n",
        "    for i, data in enumerate(progress_bar):\n",
        "        imgs = data.to(device, non_blocking=True)\n",
        "        bs = imgs.size(0)\n",
        "        real_labels = torch.full((bs,), 0.9, device=device)\n",
        "        fake_labels = torch.zeros(bs, device=device)\n",
        "        D.zero_grad()\n",
        "        outputs = D(imgs)\n",
        "        loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "        z = torch.randn(bs, z_dim, device=device)\n",
        "        fake_imgs = G(z)\n",
        "        outputs = D(fake_imgs.detach())\n",
        "        loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "        loss_D.backward()\n",
        "        opt_D.step()\n",
        "        G.zero_grad()\n",
        "        z = torch.randn(bs, z_dim, device=device)\n",
        "        fake_imgs = G(z)\n",
        "        outputs = D(fake_imgs)\n",
        "        loss_G = criterion(outputs, real_labels)\n",
        "        loss_G.backward()\n",
        "        opt_G.step()\n",
        "        epoch_loss_G += loss_G.item()\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss_D': loss_D.item(),\n",
        "            'Loss_G': loss_G.item(),\n",
        "            'Epoch': epoch+1,\n",
        "            'Step': steps+1,\n",
        "        })\n",
        "        steps += 1\n",
        "\n",
        "\n",
        "    avg_loss_G = epoch_loss_G / len(dataloader)\n",
        "    print(f'\\nEpoch {epoch+1}  GLoss: {avg_loss_G:.4f}')\n",
        "\n",
        "\n",
        "    early_stopping(avg_loss_G)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "    scheduler_D.step()\n",
        "    scheduler_G.step()\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        z_sample = torch.randn(100, z_dim, device=device)\n",
        "        f_imgs_sample = G(z_sample).cpu()\n",
        "        f_imgs_sample = (f_imgs_sample + 1) / 2.0\n",
        "        filename = os.path.join(log_dir, f'Epoch_{epoch+1:03d}.jpg')\n",
        "        torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n",
        "        print(f' | Saved sample images to {filename}.')\n",
        "\n",
        "\n",
        "        grid_img = torchvision.utils.make_grid(f_imgs_sample, nrow=10)\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.imshow(grid_img.permute(1, 2, 0).numpy())\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    G.train()\n",
        "\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        torch.save(G.state_dict(), os.path.join(ckpt_dir, f'G_epoch_{epoch+1}.pth'))\n",
        "        torch.save(D.state_dict(), os.path.join(ckpt_dir, f'D_epoch_{epoch+1}.pth'))\n",
        "        print(f' Saved model  {epoch+1}.')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rXA27oZa3a11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G.eval()\n",
        "with torch.no_grad():\n",
        "    n_output = 1000\n",
        "    batch_size_gen = 100\n",
        "    generated_images = []\n",
        "    for _ in range(n_output // batch_size_gen):\n",
        "        z = torch.randn(batch_size_gen, z_dim, device=device)\n",
        "        imgs = G(z).cpu()\n",
        "        imgs = (imgs + 1) / 2.0\n",
        "        generated_images.append(imgs)\n",
        "    generated_images = torch.cat(generated_images, dim=0)\n",
        "output_dir = 'output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "for i in range(n_output):\n",
        "    torchvision.utils.save_image(generated_images[i], os.path.join(output_dir, f'{i+1}.jpg'))\n",
        "shutil.make_archive('images', 'gztar', output_dir)\n",
        "print('saved output to  into images2.tar,gz')"
      ],
      "metadata": {
        "id": "3Bc-Whc33rxL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}